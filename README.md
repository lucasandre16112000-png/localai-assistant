<div align="center">

# ğŸ¤– LocalAI Assistant

### Premium AI Assistant with Local LLM Support

[![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](https://opensource.org/licenses/MIT)
[![Python](https://img.shields.io/badge/Python-3.11+-green.svg)](https://python.org)
[![React](https://img.shields.io/badge/React-19-61DAFB.svg)](https://reactjs.org)
[![TypeScript](https://img.shields.io/badge/TypeScript-5.3-3178C6.svg)](https://typescriptlang.org)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.109-009688.svg)](https://fastapi.tiangolo.com)
[![TailwindCSS](https://img.shields.io/badge/TailwindCSS-3.4-38B2AC.svg)](https://tailwindcss.com)
[![Docker](https://img.shields.io/badge/Docker-Ready-2496ED.svg)](https://docker.com)

<p align="center">
  <strong>Um assistente de IA moderno e de nÃ­vel enterprise com interface premium e funcionalidades poderosas.</strong>
</p>

[Funcionalidades](#-funcionalidades) â€¢ [Screenshots](#-screenshots) â€¢ [InstalaÃ§Ã£o](#-instalaÃ§Ã£o-passo-a-passo) â€¢ [Como Usar](#-como-usar) â€¢ [API](#-documentaÃ§Ã£o-da-api) â€¢ [Troubleshooting](#-troubleshooting)

</div>

---

## âœ¨ Funcionalidades

### ğŸ¨ Interface Premium
- **Dashboard Moderno** - Analytics bonitos com estatÃ­sticas em tempo real
- **Interface estilo ChatGPT** - ExperiÃªncia de chat familiar e intuitiva
- **Dark/Light Mode** - AlternÃ¢ncia elegante de temas
- **Design Glassmorphism** - TendÃªncias modernas de design 2024/2025
- **AnimaÃ§Ãµes Suaves** - InteraÃ§Ãµes com Framer Motion
- **Design Responsivo** - Mobile-first, funciona em todos os dispositivos

### ğŸ¤– Capacidades de IA
- **Suporte a LLM Local** - Execute modelos localmente com Ollama
- **MÃºltiplos Modelos** - Alterne entre diferentes modelos de IA
- **Streaming de Respostas** - Streaming de tokens em tempo real
- **ParÃ¢metros CustomizÃ¡veis** - Controles de Temperature, Top-P, Top-K
- **System Prompts** - Templates de prompts prÃ©-construÃ­dos e customizados

### ğŸ’¬ Funcionalidades do Chat
- **Gerenciamento de Conversas** - Criar, editar, deletar conversas
- **HistÃ³rico de Mensagens** - PersistÃªncia completa das conversas
- **Syntax Highlighting** - Destaque de cÃ³digo para 100+ linguagens
- **RenderizaÃ§Ã£o Markdown** - FormataÃ§Ã£o de texto rica
- **Copiar para Clipboard** - CÃ³pia de cÃ³digo com um clique

### ğŸ“Š Dashboard de Analytics
- **EstatÃ­sticas de Uso** - Acompanhe conversas, mensagens, tokens
- **Uso de Modelos** - Veja quais modelos vocÃª mais usa
- **MÃ©tricas de Performance** - Analytics de tempo de resposta
- **GrÃ¡ficos de Atividade** - PadrÃµes visuais de uso

---

## ğŸ“¸ Screenshots

<div align="center">

### Dashboard
![Dashboard](screenshots/dashboard.jpg)
*Dashboard premium de analytics com estatÃ­sticas em tempo real*

### Interface de Chat
![Chat](screenshots/chat.jpg)
*Interface de chat moderna com streaming de respostas*

### ConfiguraÃ§Ãµes
![Settings](screenshots/settings.jpg)
*ConfiguraÃ§Ãµes completas com configuraÃ§Ã£o de modelos*

</div>

---

## ğŸš€ InstalaÃ§Ã£o Passo a Passo

### PrÃ©-requisitos

Antes de comeÃ§ar, vocÃª precisa ter instalado:

| Software | VersÃ£o | Link para Download |
|----------|--------|-------------------|
| **Python** | 3.11 ou superior | [python.org/downloads](https://python.org/downloads) |
| **Node.js** | 20 ou superior | [nodejs.org](https://nodejs.org) |
| **Ollama** | Ãšltima versÃ£o | [ollama.ai](https://ollama.ai) |
| **Git** | Qualquer versÃ£o | [git-scm.com](https://git-scm.com) |
| **Docker** | (Opcional) | [docker.com](https://docker.com) |

---

### ğŸ“¥ Passo 1: Instalar o Ollama

O Ollama Ã© o software que roda os modelos de IA no seu computador.

**Windows:**
1. Acesse [ollama.ai](https://ollama.ai)
2. Clique em "Download for Windows"
3. Execute o instalador e siga as instruÃ§Ãµes
4. ApÃ³s instalar, o Ollama iniciarÃ¡ automaticamente

**macOS:**
```bash
# Via Homebrew
brew install ollama

# Ou baixe diretamente de ollama.ai
```

**Linux:**
```bash
curl -fsSL https://ollama.ai/install.sh | sh
```

---

### ğŸ“¥ Passo 2: Baixar um Modelo de IA

Abra o terminal/prompt de comando e execute:

```bash
# Modelo recomendado (leve e rÃ¡pido)
ollama pull dolphin-mistral

# Modelo para cÃ³digo (opcional)
ollama pull codellama

# Modelo sem censura (opcional)
ollama pull wizardlm-uncensored
```

> **Nota:** O download pode demorar alguns minutos dependendo da sua internet. Os modelos tÃªm entre 4GB e 8GB.

---

### ğŸ“¥ Passo 3: Clonar o RepositÃ³rio

```bash
# Clone o projeto
git clone https://github.com/lucasandre16112000-png/localai-assistant.git

# Entre na pasta do projeto
cd localai-assistant
```

---

### ğŸ“¥ Passo 4: Configurar VariÃ¡veis de Ambiente

```bash
# Copie o arquivo de exemplo
cp .env.example .env
```

Edite o arquivo `.env` se necessÃ¡rio (os valores padrÃ£o funcionam para a maioria dos casos).

---

### ğŸ“¥ Passo 5: Iniciar o Backend

**OpÃ§Ã£o A - Com Docker (Recomendado):**
```bash
docker-compose up -d
```

**OpÃ§Ã£o B - Manualmente:**

Abra um terminal e execute:
```bash
# Entre na pasta do backend
cd backend

# Crie um ambiente virtual
python -m venv venv

# Ative o ambiente virtual
# Windows:
venv\Scripts\activate
# Linux/macOS:
source venv/bin/activate

# Instale as dependÃªncias
pip install -r requirements.txt

# Inicie o servidor
uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload
```

VocÃª verÃ¡ uma mensagem como:
```
INFO:     Uvicorn running on http://0.0.0.0:8000
```

---

### ğŸ“¥ Passo 6: Iniciar o Frontend

Abra **outro terminal** (mantenha o backend rodando) e execute:

```bash
# Entre na pasta do frontend
cd frontend

# Instale as dependÃªncias
npm install
# ou se preferir pnpm:
pnpm install

# Inicie o servidor de desenvolvimento
npm run dev
# ou:
pnpm dev
```

VocÃª verÃ¡ uma mensagem como:
```
VITE v5.x.x  ready in xxx ms
âœ  Local:   http://localhost:3000/
```

---

### ğŸ“¥ Passo 7: Acessar a AplicaÃ§Ã£o

1. Abra seu navegador
2. Acesse: **http://localhost:3000**
3. Pronto! VocÃª verÃ¡ a interface do LocalAI Assistant

---

## ğŸ® Como Usar

### Iniciando uma Conversa

1. Clique em **"+ New Chat"** na sidebar
2. Digite sua pergunta no campo de texto
3. Pressione **Enter** ou clique em **Send**
4. Aguarde a resposta da IA (aparece em tempo real!)

### Exemplos de Perguntas

```
- "Explique como funciona o algoritmo QuickSort em Python"
- "Escreva uma funÃ§Ã£o para calcular o fatorial de um nÃºmero"
- "Crie um componente React para um formulÃ¡rio de login"
- "Me ajude a debugar este cÃ³digo: [cole seu cÃ³digo]"
```

### Usando o Dashboard

1. Clique em **"Dashboard"** na sidebar
2. Veja estatÃ­sticas de uso:
   - Total de conversas
   - Mensagens enviadas
   - Tokens utilizados
   - Tempo mÃ©dio de resposta

### Configurando o Modelo

1. Clique em **"Settings"** na sidebar
2. VÃ¡ em **"Models"**
3. Selecione o modelo desejado
4. Ajuste parÃ¢metros como temperatura (criatividade)

---

## ğŸ“š DocumentaÃ§Ã£o da API

### Endpoints Principais

| MÃ©todo | Endpoint | DescriÃ§Ã£o |
|--------|----------|-----------|
| `GET` | `/api/v1/conversations` | Lista todas as conversas |
| `POST` | `/api/v1/conversations` | Cria nova conversa |
| `GET` | `/api/v1/conversations/{uuid}` | ObtÃ©m conversa com mensagens |
| `DELETE` | `/api/v1/conversations/{uuid}` | Deleta conversa |
| `POST` | `/api/v1/chat/completions` | Envia mensagem e obtÃ©m resposta |
| `GET` | `/api/v1/models` | Lista modelos disponÃ­veis |

### DocumentaÃ§Ã£o Interativa

Quando o backend estiver rodando, acesse:
- **Swagger UI**: http://localhost:8000/docs
- **ReDoc**: http://localhost:8000/redoc

---

## ğŸ”§ ConfiguraÃ§Ã£o

### VariÃ¡veis de Ambiente

| VariÃ¡vel | DescriÃ§Ã£o | PadrÃ£o |
|----------|-----------|--------|
| `OLLAMA_BASE_URL` | URL da API do Ollama | `http://localhost:11434` |
| `DEFAULT_MODEL` | Modelo LLM padrÃ£o | `dolphin-mistral` |
| `DEFAULT_TEMPERATURE` | Temperatura de sampling | `0.7` |
| `DATABASE_URL` | ConexÃ£o do banco de dados | `sqlite:///./localai.db` |
| `DEBUG` | Ativar modo debug | `false` |

---

## â“ Troubleshooting

### Problema: "Ollama nÃ£o estÃ¡ respondendo"

**SoluÃ§Ã£o:**
1. Verifique se o Ollama estÃ¡ rodando:
   ```bash
   ollama list
   ```
2. Se nÃ£o estiver, inicie-o:
   ```bash
   ollama serve
   ```

### Problema: "Modelo nÃ£o encontrado"

**SoluÃ§Ã£o:**
```bash
# Baixe o modelo
ollama pull dolphin-mistral
```

### Problema: "Erro de conexÃ£o com o backend"

**SoluÃ§Ã£o:**
1. Verifique se o backend estÃ¡ rodando na porta 8000
2. Acesse http://localhost:8000/health para verificar

### Problema: "npm/pnpm nÃ£o encontrado"

**SoluÃ§Ã£o:**
1. Instale o Node.js de [nodejs.org](https://nodejs.org)
2. Reinicie o terminal apÃ³s a instalaÃ§Ã£o

### Problema: "Python nÃ£o encontrado"

**SoluÃ§Ã£o:**
1. Instale o Python de [python.org](https://python.org)
2. Marque a opÃ§Ã£o "Add Python to PATH" durante a instalaÃ§Ã£o
3. Reinicie o terminal

---

## ğŸ—ï¸ Estrutura do Projeto

```
localai-assistant/
â”œâ”€â”€ backend/                 # Servidor FastAPI
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ main.py         # AplicaÃ§Ã£o FastAPI
â”‚   â”‚   â”œâ”€â”€ routers/        # Endpoints da API
â”‚   â”‚   â”œâ”€â”€ services/       # LÃ³gica de negÃ³cio
â”‚   â”‚   â”œâ”€â”€ models/         # Modelos do banco de dados
â”‚   â”‚   â”œâ”€â”€ schemas/        # Schemas Pydantic
â”‚   â”‚   â””â”€â”€ core/           # ConfiguraÃ§Ãµes
â”‚   â”œâ”€â”€ tests/              # Testes
â”‚   â””â”€â”€ requirements.txt    # DependÃªncias Python
â”œâ”€â”€ frontend/               # AplicaÃ§Ã£o React
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/     # Componentes React
â”‚   â”‚   â”œâ”€â”€ lib/            # UtilitÃ¡rios & API
â”‚   â”‚   â”œâ”€â”€ styles/         # Estilos globais
â”‚   â”‚   â””â”€â”€ App.tsx         # Componente principal
â”‚   â””â”€â”€ package.json        # DependÃªncias Node.js
â”œâ”€â”€ screenshots/            # Screenshots do projeto
â”œâ”€â”€ docker-compose.yml      # ConfiguraÃ§Ã£o Docker
â”œâ”€â”€ .env.example            # Exemplo de variÃ¡veis de ambiente
â””â”€â”€ README.md               # Este arquivo
```

---

## ğŸ¤ Contribuindo

ContribuiÃ§Ãµes sÃ£o bem-vindas! Sinta-se Ã  vontade para enviar um Pull Request.

1. Fork o repositÃ³rio
2. Crie sua branch de feature (`git checkout -b feature/NovaFuncionalidade`)
3. Commit suas mudanÃ§as (`git commit -m 'Adiciona nova funcionalidade'`)
4. Push para a branch (`git push origin feature/NovaFuncionalidade`)
5. Abra um Pull Request

---

## ğŸ“„ LicenÃ§a

Este projeto estÃ¡ licenciado sob a LicenÃ§a MIT - veja o arquivo [LICENSE](LICENSE) para detalhes.

---

## ğŸ‘¨â€ğŸ’» Autor

**Lucas Andre S**

- GitHub: [@lucasandre16112000-png](https://github.com/lucasandre16112000-png)

---

<div align="center">

### â­ DÃª uma estrela neste repo se vocÃª achou Ãºtil!

Feito com â¤ï¸ por Lucas Andre S

</div>
